{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>What follows is the documentation for using the code and data from the manuscript  \"A serious flaw in the design of pLLM-based protein-protein interaction models\".</p> <p>The code, as well as the documentation, is divided into a few sections:</p> Section Description <code>autofigures</code> Generates all the experimental figures present in the manuscript. <code>experiments</code> Code to conduct the experiments in the manuscript. <code>data</code> Data used for the experiments in the manuscript."},{"location":"#license","title":"License","text":""},{"location":"#code","title":"Code","text":"<p>All code files in this repository are licensed under the GNU AGPLv3 License.</p> <p>Code for \"A serious flaw in the design of pLLM-based protein-protein interactions\"</p> <p>Copyright (C) 2025 Joseph Szymborski</p> <p>This program is free software: you can redistribute it and/or modify    it under the terms of the GNU Affero General Public License as    published by the Free Software Foundation, either version 3 of the    License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful,    but WITHOUT ANY WARRANTY; without even the implied warranty of    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the    GNU Affero General Public License for more details.</p> <p>You should have received a copy of the GNU Affero General Public License    along with this program.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"#data","title":"Data","text":"<p>Data for \"A serious flaw in the design of pLLM-based protein-protein interactions\" by Joseph Szymborski is licensed under CC BY-NC-SA 4.0</p>"},{"location":"glossary/","title":"Definitions &amp; Glossary","text":""},{"location":"glossary/#definitions","title":"Definitions","text":""},{"location":"glossary/#pllms","title":"pLLMs","text":"ESM Evolutionary Scale Modeling (ESM) is a protein large language model released by Meta and described in Verkuil et al.. We use the V2 650M weights released by Meta and retrieved by the PyTorch Hub. It is referred to as <code>esm</code> by the code for this manuscript. ProtBERT ProtBERT is one of the many protein large language models described by Elnaggar et al., and is based on BERT. Weights are retrieved from the HuggingFace repository Rostlab/prot_bert. It is referred to as <code>prottrans_bert</code> by the code for the manuscript. ProtT5 ProtBERT is one of the many protein large language models described by Elnaggar et al., and is based on BERT. Weights are retrieved from the HuggingFace repository Rostlab/prot_t5_xl_half_uniref50-enc. It is referred to as <code>prottrans_t5</code> by the code for the manuscript. ProSE ProSE is a protein large language models described by Bepler et al., and is based on an recurrent neural network architecture. Weights are retrieved from the GitHub repository. It is referred to as <code>prose</code> by the code for the manuscript. ProteinBERT ProteinBERT is a protein large language models described by Brandes et al., and is based on the BERT architecture. It uses an additional Gene Ontology (GO) term annotation prediction task. Weights and code were retrieved from the GitHub repository. It is referred to as <code>proteinbert</code> by the code for the manuscript. SqueezeProt-SP (Strict) SqueezeProt-SP (Strict) is a novel protein large language model introduced in this manuscript. It is trained on a strict SWISS-PROT dataset which excludes proteins from the downstream PPI testing dataset. See the manuscript for more details. It is referred to as <code>squeezeprot_sp_strict</code> by the code for the manuscript. SqueezeProt-SP (Non-strict) SqueezeProt-SP (Non-strict) is a novel protein large language model introduced in this manuscript. It is trained on a non-strict SWISS-PROT dataset which includes proteins from the downstream PPI testing dataset. See the manuscript for more details. It is referred to as <code>squeezeprot_sp_nonstrict</code> by the code for the manuscript. SqueezeProt-U50 SqueezeProt-U50 is a novel protein large language model introduced in this manuscript. It is trained on a non-strict UniRef50 dataset which includes proteins from the downstream PPI testing dataset. See the manuscript for more details. It is referred to as <code>squeezeprot_u50</code> by the code for the manuscript."},{"location":"glossary/#non-pllm-based-ppi-inference-methods","title":"Non-pLLM-based PPI Inference Methods","text":"RAPPPID RAPPPID is a non-pLLM-based PPI inference method described in Szymborski et al. and is based on an AWD-LSTM regularized neural network."},{"location":"glossary/#glossary","title":"Glossary","text":"<p>"},{"location":"autofigures/","title":"autofigures","text":"<p>This section of the documentation concerns the automatic generation of plots and graphs present in the manuscript. A command-line tool, named <code>autofigures</code>, was created to facilitate this.</p>"},{"location":"autofigures/#installation","title":"Installation","text":"<p>Read more about how to install <code>autofigures</code> on the Installation page.</p>"},{"location":"autofigures/#usage","title":"Usage","text":"<p>Read more about how to use <code>autofigures</code> on the Usage page.</p>"},{"location":"autofigures/installation/","title":"Installation","text":"<p>autofigures is Python programme and can be installed with either Poetry or pip and virtual environment.</p>"},{"location":"autofigures/installation/#poetry","title":"Poetry","text":"<p>To install dependencies using Poetry, run:</p> <p><pre><code>poetry install\n</code></pre> you can then run autofigures like so:</p> <pre><code>poetry run python -m autofigures\n</code></pre>"},{"location":"autofigures/installation/#pip","title":"pip","text":"<p>You can use pip to install all dependencies. Using a virtualenv environment is recommended.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>usage is then as simple as</p> <pre><code>python -m autofigures\n</code></pre>"},{"location":"autofigures/usage/","title":"Usage","text":"<p>Figures are generated by calling sub-commands. These sub-commands are:</p> Sub-command Description get_scores Infer and score pLLM-based PPI models. Pre-requisite for some commands. speed Make figure 1a, which compares the speed and PPI classification performance of SoTA pLLMs and SqueezeProt. strict_nonstrict Make figure 1c-d, comparing PPI classification performance of both strict and non-strict variants of SqueezeProt-SP. kw Make figure 1e-f, which reports the performance of strict and non-strict variants of SqueezeProt-SP on a UniProt Keyword annotation task. concordance Make figure 1g-h, which reports the concordance between pLLM methods and SqueezeProt-SP variants. length_histogram Make figure 2a, which shows the distribution of protein lengths. length_heatmap Make figure 2b, which shows the performance of pLLM-base PPIs as a function of the length of the proteins in the pair. acc_by_length Make figure 2c, which shows the performance of pLLM-base PPIs as a function of the longest protein in the pair. sars_cov2 Make figure 2e, which reports AUROC curves of pLLM-based PPI methods tested on both Human PPIs and Human-SARS-CoV-2 PPIs. mutation Make figure 2f, which reports the change in binding affinity as a function of the change in interaction prediction in mutated proteins."},{"location":"autofigures/usage/#output-and-data-folders","title":"Output and Data Folders","text":"<p>Common among these sub-commands are two flags: output_folder and data_folder. The former is where the results of the sub-commands are saved, and the latter is where the data used to run the sub-commands is kept. These default to \"out\" and \"data\" in the root directory if not specified.</p>"},{"location":"autofigures/usage/#get_scores","title":"get_scores","text":"<pre><code>NAME\n    autofigures get_scores\n\nSYNOPSIS\n    autofigures get_scores &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description","title":"Description","text":"<p>The get_scores command:</p> <ol> <li>Loads already-trained pLLM-based PPI models.</li> <li>Infer interaction probabilities of the C3 PPI testing dataset using the loaded PPI models.</li> <li>Score the inferred probabilities using MCC and AUROC.</li> </ol> <p>This command does not generate a figure, but running it a pre-requisite for other commands.</p> <p>The checkpoints for the pLLM-based PPI models are stored in the data folder (<code>data/chkpts</code>), and were generated by the ppi_bench experiment.</p> <p>get_scores also needs pre-computed embeddings for 6251a85a-47d0-11ee-be56-0242ac120002:a7359539c27837c8906ab212af07e0a8:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:e61e4281785870f7178232642792545f:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:c7f1ada6f7bab4ad2d1a28e62e4fc382:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:b15952cf6f01367f42061bef3cfc1663:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:5361e29c6576c68a65cdd85bf90e6548:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:9344ca2654138ea7c7ad6fba6cbcdb4f:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:584edec030f8760efd4c6ad493bde0c0:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:d1da40ed80b68065d3b197976fc3e571:&lt;__None__&gt;.</p> <p>The data used for this is from our previous manuscript \"INTREPPPID\u2014an orthologue-informed quintuplet network for cross-species prediction of protein\u2013protein interaction\". They are pairs of Human interactions from STRING that conform to Park &amp; Marcotte's C3 criteria. We use the 8675309 seed for this study (extra points if you know where the seed is from).</p>"},{"location":"autofigures/usage/#flags","title":"Flags","text":"Long Flag Short Flag Default Description --output_folder -o None Location of the output folder. --data_folder -d None Location of the data folders."},{"location":"autofigures/usage/#speed","title":"speed","text":"<pre><code>NAME\n    __main__.py speed\n\nSYNOPSIS\n    __main__.py speed &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -y, --y_metric=Y_METRIC\n        Type: str\n        Default: 'mcc'\n</code></pre>"},{"location":"autofigures/usage/#description_1","title":"Description","text":"<p>Make figure 1a, which compares the speed and PPI classification performance of SoTA pLLMs and SqueezeProt.</p> <p>This command will save the plot to disk as <code>figures/speed.sv</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_1","title":"Flags","text":"Long Flag Short Flag Default Description --output_folder -o None Location of the output folder. --data_folder -d None Location of the data folders. --y_metric -y \"mcc\" What PPI classification metric should be reported on the y-axis."},{"location":"autofigures/usage/#strict_nonstrict","title":"strict_nonstrict","text":"<pre><code>NAME\n    __main__.py strict_nonstrict\n\nSYNOPSIS\n    __main__.py strict_nonstrict &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -a, --add_markers=ADD_MARKERS\n        Type: bool\n        Default: True\n    -f, --first_metric=FIRST_METRIC\n        Type: str\n        Default: 'auroc'\n    -s, --second_metric=SECOND_METRIC\n        Type: str\n        Default: 'mcc'\n</code></pre>"},{"location":"autofigures/usage/#description_2","title":"Description","text":"<p>The strict_nonstrict command creates the Fig. 1c-d from the manuscript, which compares the performance of strict and non-strict variants of SqueezeProt-SP.</p> <p>Once it has run, it will save the plot as <code>figure/strict_nonstrict.svg</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_2","title":"Flags","text":"Long Flag Short Flag Default Description --output_folder -o None Location of the output folder. --data_folder -d None Location of the data folders. --add_markers -a True Whether to add markers to the plot indicating replicate performances. Otherwise, whiskers are plotted to show standard deviation. --first_metric -f \"auroc\" Measure performance using which metric in the left panel. Must be one of \"mcc\", \"auroc\", \"ap\", or \"f1\". --second_metric -s \"mcc\" Measure performance using which metric in the left panel. Must be one of \"mcc\", \"auroc\", \"ap\", or \"f1\"."},{"location":"autofigures/usage/#kw","title":"kw","text":"<pre><code>NAME\n    autofigures kw\n\nSYNOPSIS\n    autofigures kw &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_3","title":"Description","text":"<p>Make figure 1e-f, which reports the performance of strict and non-strict variants of SqueezeProt-SP on a UniProt Keyword annotation task.</p> <p>Once it has run, it will save the plot as <code>figure/kw.svg</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_3","title":"Flags","text":"Long Flag Short Flag Defaults Description --output_folder -o None Location of the output folder. --data_folder -d None Location of the data folders."},{"location":"autofigures/usage/#concordance","title":"concordance","text":"<pre><code>NAME\n    autofigures concordance\n\nSYNOPSIS\n    autofigures concordance &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -c, --cohen_kappa=COHEN_KAPPA\n        Type: bool\n        Default: False\n</code></pre>"},{"location":"autofigures/usage/#description_4","title":"Description","text":"<p>Make figure 1g-h, which reports the concordance between pLLM methods and SqueezeProt-SP variants.</p> <p>Once it has run, it will save the plot as <code>figure/concordance.svg</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_4","title":"Flags","text":"Long Flag Short Flag Default Description --output_folder -o None Location of the output folder. --data_folder -d None Location of the data folders --cohen_kappa -c False Display Cohen's Kappa. If False, shows skill-normalized concordance."},{"location":"autofigures/usage/#length_histogram","title":"length_histogram","text":"<pre><code>NAME\n    autofigures length_histogram\n\nSYNOPSIS\n    autofigures length_histogram &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_5","title":"Description","text":"<p>Make figure 2a, which shows the distribution of protein lengths.</p> <p>Once it has run, it will save the plot as <code>figure/length_histogram.svg</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_5","title":"Flags","text":"Long Flag Short Flag Description --output_folder -o Location of the output folder. --data_folder -d Location of the data folders."},{"location":"autofigures/usage/#length_heatmap","title":"length_heatmap","text":"<pre><code>NAME\n    autofigures length_heatmap\n\nSYNOPSIS\n    autofigures length_heatmap &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_6","title":"Description","text":"<p>Make figure 2a, which shows the distribution of protein lengths.</p> <p>Once it has run, it will save the plot the following plots in the output directory:</p> <ul> <li><code>figures/length_heatmap_dscript.svg</code></li> <li><code>figures/length_heatmap_esm.svg</code></li> <li><code>figures/length_heatmap_intrepppid.svg</code></li> <li><code>figures/length_heatmap_pipr.svg</code></li> <li><code>figures/length_heatmap_prose.svg</code></li> <li><code>figures/length_heatmap_proteinbert.svg</code></li> <li><code>figures/length_heatmap_prottrans_bert.svg</code></li> <li><code>figures/length_heatmap_prottrans_t5.svg</code></li> <li><code>figures/length_heatmap_rapppid.svg</code></li> <li><code>figures/length_heatmap_richoux.svg</code></li> <li><code>figures/length_heatmap_sizes.svg</code></li> <li><code>figures/length_heatmap_squeezeprot_sp_nonstrict.svg</code></li> <li><code>figures/length_heatmap_squeezeprot_sp_strict.svg</code></li> <li><code>figures/length_heatmap_squeezeprot_u50.svg</code></li> </ul>"},{"location":"autofigures/usage/#flags_6","title":"Flags","text":"Long Flag Short Flag Description --output_folder -o Location of the output folder. --data_folder -d Location of the data folders."},{"location":"autofigures/usage/#acc_by_length","title":"acc_by_length","text":"<pre><code>NAME\n    autofigures acc_by_length\n\nSYNOPSIS\n    autofigures acc_by_length &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_7","title":"Description","text":"<p>Make figure 2c, which shows the performance of pLLM-base PPIs as a function of the longest protein in the pair.</p> <p>Once it has run, it will save the plot as <code>figure/acc_by_length.svg</code> in the output directory.</p>"},{"location":"autofigures/usage/#flags_7","title":"Flags","text":"Long Flag Short Flag Description --output_folder -o Location of the output folder. --data_folder -d Location of the data folders."},{"location":"autofigures/usage/#sars_cov2","title":"sars_cov2","text":"<pre><code>NAME\n    autofigures sars_cov2\n\nSYNOPSIS\n    autofigures sars_cov2 &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_8","title":"Description","text":"<p>Make figure 2e, which reports AUROC curves of pLLM-based PPI methods tested on both Human PPIs and Human-SARS-CoV-2 PPIs.</p> <p>Once it has run, it will save the plot the following plots in the output directory:</p> <ul> <li><code>figures/sars_cov2_breakdown.svg</code></li> <li><code>figures/sars_cov2_roc_highlight.svg</code></li> <li><code>figures/sars_cov2_roc.svg</code></li> </ul>"},{"location":"autofigures/usage/#flags_8","title":"Flags","text":"Long Flag Short Flag Description --output_folder -o Location of the output folder. --data_folder -d Location of the data folders."},{"location":"autofigures/usage/#mutation","title":"mutation","text":"<pre><code>NAME\n    autofigures mutation\n\nSYNOPSIS\n    autofigures mutation &lt;flags&gt;\n\nFLAGS\n    -o, --output_folder=OUTPUT_FOLDER\n        Type: Optional[Union]\n        Default: None\n    -d, --data_folder=DATA_FOLDER\n        Type: Optional[Union]\n        Default: None\n</code></pre>"},{"location":"autofigures/usage/#description_9","title":"Description","text":"<p>Make figure 2f, which reports the change in binding affinity as a function of the change in interaction prediction in mutated proteins.</p> <p>Once it has run, it will save the plot the following plots in the output directory:</p> <ul> <li><code>figures/mutation_esm.png</code></li> <li><code>figures/mutation_prose.png</code></li> <li><code>figures/mutation_proteinbert.png</code></li> <li><code>figures/mutation_prottrans_bert.png</code></li> <li><code>figures/mutation_prottrans_t5.png</code></li> <li><code>figures/mutation_squeezeprot_sp_nonstrict.png</code></li> <li><code>figures/mutation_squeezeprot_sp_strict.png</code></li> <li><code>figures/mutation_squeezeprot_u50.png</code></li> </ul>"},{"location":"autofigures/usage/#flags_9","title":"Flags","text":"Long Flag Short Flag Description --output_folder -o Location of the output folder. --data_folder -d Location of the data folders."},{"location":"data/","title":"Data","text":"<p>The data folder contains data files used in the manuscript. It is subdivided into the following folders:</p> Section Description chkpts Where model checkpoints are stored. embeddings Where protein embedding databases are stored. kw Where protein embedding databases are stored. lengths Where data for the length analysis. mutation Where data for the mutation analysis. pllm Where PPI datasets are stored. ppi Where PPI datasets are stored. tokenizer Where tokenizers are stored."},{"location":"data/#chkpts","title":"chkpts","text":"<p>This folder contains model checkpoints for the SqueezeProt variants,  6251a85a-47d0-11ee-be56-0242ac120002:9159312df955d3df62202d26fabe6a88:&lt;__None__&gt;, and 6251a85a-47d0-11ee-be56-0242ac120002:0b92a5db23036f1585f3f502772d3397:&lt;__None__&gt;.</p> Folder Description <code>squeezeprot-sp.non-strict/checkpoint-1390896</code> Weights for 6251a85a-47d0-11ee-be56-0242ac120002:5641c4c6aef3617ccfaf1b18efcce5d3:&lt;__None__&gt;. <code>squeezeprot-sp.strict/checkpoint-1383824</code> Weights for 6251a85a-47d0-11ee-be56-0242ac120002:eea3b489c433af6449a610cd8d1752bc:&lt;__None__&gt;. <code>squeezeprot-sp.u50/checkpoint-2477920</code> Weights for 6251a85a-47d0-11ee-be56-0242ac120002:0bcb9efa6b21178d5c490c8dbe979bdc:&lt;__None__&gt;. <code>sars_cov2</code> Various weights for pLLM-based PPI models used in the SARS-CoV-2 analysis. <code>rapppid/1690837077.519848_red-dreamy.ckpt</code> Weights for 6251a85a-47d0-11ee-be56-0242ac120002:4712bccc761ab7c07187c9a92ab985b1:&lt;__None__&gt;. These weights were previously published in Szymborski et al.. <code>prose/prose_dlm_3x1024.sav</code> Weights for 6251a85a-47d0-11ee-be56-0242ac120002:b922d8d61c2a7c6e81071548ee777595:&lt;__None__&gt;. These weights were previously published in Bepler et al.. <code>ppi</code> Various weights for pLLM-based PPI models. <code>kw/logs/KeywordNet</code> SqueezeProt-SP keyword annotation models. <p>This folder also contains sub-folders <code>ppi</code>, <code>sars_cov2</code> where checkpoints of the pLLM-based PPI inference models are stored. Checkpoints are stored according to their model names, which are hashes of the model hyperparameters. You can see all the hyper-parameters in the <code>hparams.json</code> file within the model's folder.</p> Mappings from Model IDs to their Hyperparameters Model ID pLLM Seed Input Dimension <code>iRcKUvufAzeh9CuJAtFMgjRq8Yo=</code> 6251a85a-47d0-11ee-be56-0242ac120002:c8da3bea52fdc3d6e6e92784aab9df8c:&lt;__None__&gt; 1 6165 <code>aLK3NQAPP9uHfEDhrBCaFXTjE9I=</code> 6251a85a-47d0-11ee-be56-0242ac120002:d9bc39c2f38ba84e7d1d1d93e65f34ea:&lt;__None__&gt; 2 6165 <code>x5i3FQ9dA3iI6IoHUyCzgydVWgM=</code> 6251a85a-47d0-11ee-be56-0242ac120002:82b82b539ba17ada3e1fceeac5b59c84:&lt;__None__&gt; 3 6165 <code>DoetbgdKMxDuKdfRWOUHSGAXTf0=</code> 6251a85a-47d0-11ee-be56-0242ac120002:6839a91d8b9712b750c080f250beaf52:&lt;__None__&gt; 1 1024 <code>jjvmU1efEjj1VWqJ8frthcAuiK4=</code> 6251a85a-47d0-11ee-be56-0242ac120002:aa4191b8d2fd0a7cdf60b3a97f4fe6fb:&lt;__None__&gt; 2 1024 <code>nHLZTP7wTAmtJSlpLtAYOH6gAj0=</code> 6251a85a-47d0-11ee-be56-0242ac120002:7d14ac2800daecb864917f4379ad369a:&lt;__None__&gt; 3 1024 <code>Dvy3BYf00JQ18SHKOSdCu9zWojs=</code> 6251a85a-47d0-11ee-be56-0242ac120002:6cdfc819dbaeeb8bcece874a979fb48f:&lt;__None__&gt; 1 1024 <code>XAnrmSjztabYNXRBbdrU2uV8XRM=</code> 6251a85a-47d0-11ee-be56-0242ac120002:5d159b92878bd88977e5b825add81273:&lt;__None__&gt; 2 1024 <code>PUoe1MerYJArEl8h4d-P40V06zA=</code> 6251a85a-47d0-11ee-be56-0242ac120002:68d96576afa98a69bd7725a3b57eac67:&lt;__None__&gt; 3 1024 <code>xSIEaL28YQW14K0UhNoWmwX-_HU=</code> 6251a85a-47d0-11ee-be56-0242ac120002:ed32caca5e5af6bb72a9d328f23875a0:&lt;__None__&gt; 1 6165 <code>7KwnY62-2a7UBaKz3jplmvRqSWk=</code> 6251a85a-47d0-11ee-be56-0242ac120002:659832efca88715befa99e5ca57f411e:&lt;__None__&gt; 2 6165 <code>3VHXtNHqEashvmxcWsXwCxgPqs0=</code> 6251a85a-47d0-11ee-be56-0242ac120002:a6273998bbf1445e470d90d729ecff46:&lt;__None__&gt; 3 6165 <code>048O3lE7pCo4Y_qpQAZLfxYrz6Q=</code> 6251a85a-47d0-11ee-be56-0242ac120002:178d9d12d16959917c4beed3f1eeec70:&lt;__None__&gt; 1 1562 <code>Ir_BXqrPDOutyG20qLfc2Sn4qoE=</code> 6251a85a-47d0-11ee-be56-0242ac120002:9d7c33428c3a54835b3d4bc9bf7ce8b8:&lt;__None__&gt; 2 1562 <code>7x1W0IhBtFMoEdgUYTJCY6yuhoc=</code> 6251a85a-47d0-11ee-be56-0242ac120002:f363e9877fd6223d350099bfc77f499c:&lt;__None__&gt; 3 1562 <code>c-HcTcy0NwO7JY8U3lu8Nva7d7o=</code> 6251a85a-47d0-11ee-be56-0242ac120002:10208d50758e873debfe4f07c550be33:&lt;__None__&gt; 1 768 <code>hdhFlXCydBsLZrIicCM4LMrNWoE=</code> 6251a85a-47d0-11ee-be56-0242ac120002:ef86c3cf3e09816841b9546b82f6651f:&lt;__None__&gt; 2 768 <code>r8TQa4FnoUdDvr2LPGZqZk4z6y4=</code> 6251a85a-47d0-11ee-be56-0242ac120002:d5eae44ab33f3b3d2775dd0c701adb02:&lt;__None__&gt; 3 768 <code>vFjXUGbR0vMEu8Bu0j2C2445J2A=</code> 6251a85a-47d0-11ee-be56-0242ac120002:ad0b0668b363d834808ee2445387ee6d:&lt;__None__&gt; 4 768 <code>5yEzfPl2E2eT54OJXLF0K75z_3I=</code> 6251a85a-47d0-11ee-be56-0242ac120002:7743ed68c88096c95b1b85a90eb89519:&lt;__None__&gt; 5 768 <code>W9DvdTJ7bW1GaCV3TyJ7HZznwZw=</code> 6251a85a-47d0-11ee-be56-0242ac120002:0e57d178786906317dc4d2bb2cdd14b0:&lt;__None__&gt; 6 768 <code>ngRDPuiaLHeCXQJvigyMuE1tsmw=</code> 6251a85a-47d0-11ee-be56-0242ac120002:9de20f7639e64a0e5fb9a8855cb6f53e:&lt;__None__&gt; 7 768 <code>MFwIN5YJ_98AUYYGORMlCvgv8j4=</code> 6251a85a-47d0-11ee-be56-0242ac120002:58e8677f380a4227c7c8f72dd65123d7:&lt;__None__&gt; 8 768 <code>yGeHPlv6AviEUwC4jssZy-FuTBM=</code> 6251a85a-47d0-11ee-be56-0242ac120002:448fd0319eead3e41a024657a2f78fd4:&lt;__None__&gt; 9 768 <code>s3SMyXYEAmNECJgqSfhKutisFLc=</code> 6251a85a-47d0-11ee-be56-0242ac120002:7001ec1debe33e0534b754ce7576b471:&lt;__None__&gt; 10 768 <code>0KCyeB_K-ZCIlNwpl3rRopZyM0I=</code> 6251a85a-47d0-11ee-be56-0242ac120002:80b75a2e8bfaa13514d3ab1601285d57:&lt;__None__&gt; 1 768 <code>1c-2iUOfs3Ye_pV0WGtCApV1Rgg=</code> 6251a85a-47d0-11ee-be56-0242ac120002:e8433a7e92812dcb21642d388cd5c45b:&lt;__None__&gt; 2 768 <code>8F6cxSJIb1syPfiZDK04DgCaP90=</code> 6251a85a-47d0-11ee-be56-0242ac120002:ad032bbcdf3269e40f53a0185f39ccad:&lt;__None__&gt; 3 768 <code>SsTLBBEPiGnpX91hunP9-E8w6jY=</code> 6251a85a-47d0-11ee-be56-0242ac120002:ced4c495876337198eb3a4f76b7fd355:&lt;__None__&gt; 4 768 <code>EdJtO7pKkYBjeeGajJ33-hySWfE=</code> 6251a85a-47d0-11ee-be56-0242ac120002:7e694e49434319085d428a3d9ee75824:&lt;__None__&gt; 5 768 <code>K51CpKE1he98de-DV7Pq67DE4ok=</code> 6251a85a-47d0-11ee-be56-0242ac120002:bb3e9e21e5976ea654005f3165ace653:&lt;__None__&gt; 6 768 <code>6U8njyd40iXoaBCx4WdL57FOX9o=</code> 6251a85a-47d0-11ee-be56-0242ac120002:1e12c08ffe2477074484b2342e522f00:&lt;__None__&gt; 7 768 <code>7YxAquqSeLtB1I85ItoXQvtGsg8=</code> 6251a85a-47d0-11ee-be56-0242ac120002:41dc7f92c297c5db4f111e48b58dc5ba:&lt;__None__&gt; 8 768 <code>qI4P2gmDPMvY6epwa6cIgG9PQlE=</code> 6251a85a-47d0-11ee-be56-0242ac120002:de8dde9323855077e013c4943a6b1edc:&lt;__None__&gt; 9 768 <code>MmCcff1PHh0lOvl84LiytfrznMw=</code> 6251a85a-47d0-11ee-be56-0242ac120002:4ebdb3153fc00ff551c49de69b391ace:&lt;__None__&gt; 10 768 <code>uARWusUXomAV5qa0X9e2V4C5wwI=</code> 6251a85a-47d0-11ee-be56-0242ac120002:498a367538c685853bd33cb4d002ebef:&lt;__None__&gt; 1 768 <code>cpns6_wtKs93e6ekMXxkBilEGv4=</code> 6251a85a-47d0-11ee-be56-0242ac120002:55b404ba56a5090107e0c9451bc4d482:&lt;__None__&gt; 2 768 <code>OcZKrg-rYN5RsfC6TH9btqfdVV8=</code> 6251a85a-47d0-11ee-be56-0242ac120002:a814cce9039e94c972e613cfea2af0b3:&lt;__None__&gt; 3 768"},{"location":"data/#embeddings","title":"embeddings","text":"<p>This folder holds pre-computed UniProt protein embeddings for each model. Embeddings are stored in LMDB databases.</p> Database pLLM <code>esm.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:7cb0a8cbc790ed89bd11246dc1d2ac03:&lt;__None__&gt; <code>prose.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:4a184840a7f44dd8364aa473c335b9de:&lt;__None__&gt; <code>proteinbert.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:706dc3bd88d8844f648b1e3ef5895833:&lt;__None__&gt; <code>prottrans_bert.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:78f3c6d1ba58aa9337d9c65754f7973f:&lt;__None__&gt; <code>prottrans_t5.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:7bd60cfb8d16091f84fc1113bd38448c:&lt;__None__&gt; <code>rapppid.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:078c56b1ccfcf4ce67314b658eb3bf3f:&lt;__None__&gt; <code>squeezeprot-sp.nonstrict.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:22c04fe992ab847aefb1fe3323eebf33:&lt;__None__&gt; <code>squeezeprot_sp_strict.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:3c0d2ac3d2aa00330235b2e6997a8d5f:&lt;__None__&gt; <code>squeezeprot_u50.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:f022fbb57ff381b7adc0c85ee00e8a45:&lt;__None__&gt; <p>Sub-folders <code>mutation</code> and <code>sars-cov-2</code> include embeddings for mutated proteins from ELASPIC and SARS-CoV-2 proteins, respectively.</p>"},{"location":"data/#kw","title":"kw","text":"<p>This folder contains data for the UniProt Keyword annotations.</p> Folder Description <code>hparams</code> Log of hyperparameters for the keyword models trains. <code>out</code> Inferred probabilities on a testing dataset. Filename corresponds to model number. <code>kw.json.gz</code> List of UniProt Keywords. <code>kw_ids.json</code> Matching UniProt Keywords to their categories. <code>kw_meta.json</code> Number of instances for each UniProt Keyword. <code>kw_train.csv</code> Contains UniProt accession numbers and the associated keywords and sequences for training the annotation model. <code>kw_test.csv</code> Contains UniProt accession numbers and the associated keywords and sequences for testing the annotation model. <code>kw_train_vecs.csv</code> Contains UniProt accession numbers and the associated keywords, sequences, and embeddings for training the annotation model. <code>kw_test_vecs.csv</code> Contains UniProt accession numbers and the associated keywords, sequences, and embeddings for testing the annotation model."},{"location":"data/#lengths","title":"lengths","text":"<p>This folder contains data for the protein length analyses.</p> Folder Description <code>length_histogram.csv.gz</code> Data for the length histogram."},{"location":"data/#mutation","title":"mutation","text":"<p>This folder contains data for the mutation analyses.</p> Folder Description <code>elaspic-trainin-set-interface-ids.csv</code> Data from the ELASPIC2 manuscript."},{"location":"data/#pllm","title":"pllm","text":"<p>This folder contains data used to train the SqueezeProt-SP pLLMs.</p> Folder Description <code>uniprot_sprot.nonstrict.eval.fasta</code> FASTA file containing sequences of proteins which are assigned to the evaluation split of the non-strict SqueezeProt-SP model. <code>uniprot_sprot.nonstrict.eval.txt</code> Text file containing sequences of proteins which are assigned to the evaluation split of the non-strict SqueezeProt-SP model. <code>uniprot_sprot.nonstrict.train.fasta</code> FASTA file containing sequences of proteins which are assigned to the training split of the non-strict SqueezeProt-SP model. <code>uniprot_sprot.nonstrict.train.txt</code> Text file containing sequences of proteins which are assigned to the training split of the non-strict SqueezeProt-SP model."},{"location":"data/#ppi","title":"ppi","text":"<p>The PPI datasets used in the manuscript are present in this folder. They are:</p> File Description <code>rapppid_[common_string_9606.protein.links.detailed.v12.0_upkb.csv]_Mz70T9t-4Y-i6jWD9sEtcjOr0X8=.h5</code> Human PPI dataset from Szymborski et al.. <code>sars-cov-2/baits.fasta</code> Sequences of bait proteins from Gordon et al.. <code>sars-cov-2/preys.fasta</code> Sequences of prey proteins from Gordon et al.. <code>sars-cov-2/covid_ppi.csv</code> Human/SARS-CoV-2 PPI pairs from Gordon et al.."},{"location":"data/#tokenizer","title":"tokenizer","text":"<p>This folder contains the tokenizer used for the SqueezeProt variants.</p> Folder Description <code>tokenizer/bert-based-cased</code> A tokenizer for the SqueezeProt variants. <code>tokenizer/rapppid</code> A tokenizer for the RAPPPID model."},{"location":"experiments/","title":"Experiments","text":"<p>The code for the experiments run in the manuscript are present in the <code>experiments</code> folder. The <code>experiments</code> folder is further divided into the following parts:</p> Section Description <code>encode_llm</code> Common API for encoding amino acid sequences with various pLLMs. <code>embedding_dbs</code> Scripts for generating databases of amino acid embeddings. <code>speed_bench</code> Scripts for benchmarking the speed of the various pLLMs. <code>ppi_bench</code> Scripts for training pLLM-based PPI inference models. <code>kw_bench</code> Scripts for training the protein annotation model. <code>sars_bench</code> Scripts for inferring SARS-CoV-2 v. Human PPI interactions. <code>mutation_bench</code> Scripts for calculating embeddings of mutants for the mutation analysis."},{"location":"experiments/embedding_dbs/","title":"embedding_dbs","text":"<p>Warning</p> <p>This section involves juggling conflicting requirements across a large number of protein embedding models. If you want to avoid this, you can just skip this step as embeddings have been pre-computed for you.</p> <p>This folder holds all the tools required to build databases of embeddings generated by a number of methods. These databases are used by many of the experiments conducted in the manuscript.</p>"},{"location":"experiments/embedding_dbs/#embed_batchpy","title":"embed_batch.py","text":"<p>This folder contains the <code>embed_batch.py</code> python script, which embeds the sequences of the proteins found in our downstream PPI dataset.</p> <pre><code>python embed_batch.py PLLM INPUT_PATH MAX_BATCH_SIZE DEVICE OUTPUT\n</code></pre>"},{"location":"experiments/embedding_dbs/#arguments","title":"Arguments","text":""},{"location":"experiments/embedding_dbs/#pllm","title":"PLLM","text":"<p>Specify the pLLM to use to embed protein sequences. The valid options are:</p> Command pLLM <code>squeezeprot_sp_nonstrict</code> 6251a85a-47d0-11ee-be56-0242ac120002:8ddbfeb54bd8c70c0320f2709efc9b22:&lt;__None__&gt; <code>squeezeprot_sp_strict</code> 6251a85a-47d0-11ee-be56-0242ac120002:6a509d36c7535122046a2fbbe9524dce:&lt;__None__&gt; <code>squeezeprot_u50</code> 6251a85a-47d0-11ee-be56-0242ac120002:05c65da403d7abdabfd984d77a5b68c9:&lt;__None__&gt; <code>prottrans_bert</code> 6251a85a-47d0-11ee-be56-0242ac120002:c2d5e08e50429f7c3914d5291b5d2e24:&lt;__None__&gt; <code>prottrans_t5</code> 6251a85a-47d0-11ee-be56-0242ac120002:d8b22c8694357e38dc6c173e273b37e2:&lt;__None__&gt; <code>proteinbert</code> 6251a85a-47d0-11ee-be56-0242ac120002:fb9168a49ae6c2838262471e320a947a:&lt;__None__&gt; <code>prose</code> 6251a85a-47d0-11ee-be56-0242ac120002:f4bd99e1ca3981a277ca6cd77c3f1140:&lt;__None__&gt; <code>esm</code> 6251a85a-47d0-11ee-be56-0242ac120002:50f350042ee758d9dbfe83ac834203ba:&lt;__None__&gt; <code>rapppid</code> 6251a85a-47d0-11ee-be56-0242ac120002:3357f962d076553741173662ae74bd62:&lt;__None__&gt;"},{"location":"experiments/embedding_dbs/#input_path","title":"INPUT_PATH","text":"<p>This is the file path to the protein sequences. Valid file formats are:</p> Format Required Extension FASTA <code>.fasta</code> Gzip'd FASTA <code>.fasta.gz</code> RAPPPID PPI Dataset <code>.h5</code> CSV <code>.csv</code> <p>The CSV format must contain two columns, with the headers \"accession\" and \"sequence\", which must contain a protein ID (usually UniProt accession) and the amino acid sequence, respectively. Ultimately, it should look something like:</p> <pre><code>accession,sequence\nP05067,MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGR...\nQ29537,MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAV...\n</code></pre> <p>Tip</p> <p>To encode the sequences from the PPI dataset used in the manuscript, just point this to <code>../../data/ppi/rapppid_[common_string_9606.protein.links.detailed.v12.0_upkb.csv]_Mz70T9t-4Y-i6jWD9sEtcjOr0X8=.h5</code>.</p>"},{"location":"experiments/embedding_dbs/#max_batch_size","title":"MAX_BATCH_SIZE","text":"<p>The size of the batches of sequences to embed at a time. This will increase RAM/VRAM usage.</p>"},{"location":"experiments/embedding_dbs/#device","title":"DEVICE","text":"<p>Either \"cpu\" or \"cuda\" depending on whether sequences are to be embedded on the CPU or using a GPU. Defaults to \"cpu\".</p>"},{"location":"experiments/embedding_dbs/#output","title":"OUTPUT","text":"<p>Path to save the embeddings database to. The default paths change with the pLLMs you specify:</p> pLLM Path 6251a85a-47d0-11ee-be56-0242ac120002:7bbbbd0f470c22d3774533fd06c347cf:&lt;__None__&gt; <code>../../data/embeddings/squeezeprot_sp_nonstrict.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:bf84c159d8328676fc0e3c804631eb34:&lt;__None__&gt; <code>../../data/embeddings/squeezeprot_sp_strict.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:b7b0856155578f40d695a98f06b84fe5:&lt;__None__&gt; <code>../../data/embeddings/squeezeprot_u50.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:df7130f86d608994bfe6a0bf1202b0e3:&lt;__None__&gt; <code>../../data/embeddings/prottrans_bert.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:b85a9f0e0c1ef486ae0de54cb9646a48:&lt;__None__&gt; <code>../../data/embeddings/prottrans_t5.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:cf8b512e2e1a9ba402b3be49e490732d:&lt;__None__&gt; <code>../../data/embeddings/proteinbert.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:12d15804b5105edca3cee8c6c35fa83d:&lt;__None__&gt; <code>../../data/embeddings/prose.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:1260176079637d0aa5daf518ef56bf73:&lt;__None__&gt; <code>../../data/embeddings/esm.lmdb</code> 6251a85a-47d0-11ee-be56-0242ac120002:d1f83b55d945492a49bb6e6133b85860:&lt;__None__&gt; <code>../../data/embeddings/rapppid.lmdb</code>"},{"location":"experiments/embedding_dbs/#requirements","title":"Requirements","text":"<p>Because some models have conflicting dependencies, we need to create different environments for different models. You will then need to activate/deactivate those environments when running <code>embed_batch.py</code> depending on the model you are using.</p>"},{"location":"experiments/embedding_dbs/#rapppid","title":"RAPPPID","text":"<p>RAPPPID works well with Python 3.8, and the dependencies are in the file <code>requirements_rapppid.txt</code>.</p> <p>More information can be found on the RAPPPID GitHub repository.</p>"},{"location":"experiments/embedding_dbs/#proteinbert","title":"ProteinBERT","text":"<p>ProteinBERT works well with Python 3.6, and the dependencies are in the file <code>requirements_proteinbert.txt</code>. You must then install ProteinBERT proper by cloning the repository and installing thusly:</p> <pre><code>git clone https://github.com/nadavbra/protein_bert\ncd protein_bert\ngit submodule init\ngit submodule update\npython setup.py install\n</code></pre> <p>More information can be found on the ProteinBERT GitHub repository.</p>"},{"location":"experiments/embedding_dbs/#prose","title":"ProSE","text":"<p>ProSE works well with Python 3.7, and the dependencies are in the file <code>requirements_prose.txt</code>.</p>"},{"location":"experiments/embedding_dbs/#all-other-models","title":"All other models","text":"<p>For all other models, we recommend Python 3.11. The dependencies are in the file <code>requirements.txt</code>.</p>"},{"location":"experiments/encode_llm/","title":"encode_llm","text":"<p>This module exposes a common API for inferring the embeddings used in this manuscript from amino acid sequences using the pLLMs, as well as 6251a85a-47d0-11ee-be56-0242ac120002:6f5862880a850bfeacc8b9782d979ae9:&lt;__None__&gt;, which is not a language model but from which embeddings can be had.</p> <p>There are sub-modules for one of each of the pLLMs and 6251a85a-47d0-11ee-be56-0242ac120002:95c17e4658ace091a573c94f8b904601:&lt;__None__&gt;, which are:</p> Sub-module pLLM <code>esmer.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:828febf579297b15eb0ff59bd2839f89:&lt;__None__&gt; <code>proser.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:1b77467e3775d534924131510d4f3f9e:&lt;__None__&gt; <code>proteinberter.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:8beaa172d3b3879df6e3b51169eea3a6:&lt;__None__&gt; <code>prottrans_bert.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:a5170c4f192387b213ad00918337e2ac:&lt;__None__&gt; <code>prottrans_t5.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:5d8331738bba0085ea22877d1a6a9b9c:&lt;__None__&gt; <code>rapppid.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:6995541f94d3acdfe84aafb7104c360a:&lt;__None__&gt; <code>squeezebert.py</code> 6251a85a-47d0-11ee-be56-0242ac120002:a4f6462cc7b41cfe38d49c4387956952:&lt;__None__&gt;, 6251a85a-47d0-11ee-be56-0242ac120002:584edec030f8760efd4c6ad493bde0c0:&lt;__None__&gt;,6251a85a-47d0-11ee-be56-0242ac120002:92331ea3024b1f61c8e94d9cbcb56916:&lt;__None__&gt;"},{"location":"experiments/encode_llm/#common-api","title":"Common API","text":"<p>All modules contain two functions, encode and encode_batch for calculating the embeddings from one single sequence or many, respectively.</p>"},{"location":"experiments/encode_llm/#encode","title":"encode","text":"<pre><code>encode(sequence: str, device: str = \"cpu\") -&gt; List[float]\n</code></pre> Argument Default Type Description <code>sequence</code> None String Amino-acid sequence to embed. <code>device</code> \"cpu\" String Which device to run this on. Must be a valid PyTorch device string."},{"location":"experiments/encode_llm/#encode_batch","title":"encode_batch","text":"<pre><code>encode_batch(batch: List[str], device: str = 'cpu') -&gt; List[List[float]]\n</code></pre> Argument Default Type Description <code>batch</code> None List[str] A list of amino-acid sequences to embed. <code>device</code> \"cpu\" String Which device to run this on. Must be a valid PyTorch device string."},{"location":"experiments/encode_llm/#selecting-squeezeprot-variants","title":"Selecting SqueezeProt Variants","text":"<p>There is only one module for all the SqueezeProt model, but there are three variants. You can use two additional arguments to specify the weights and tokenizer used to embed sequences. To embed with a specific variant, simply point those arguments to the paths of the corresponding weights for those variants.</p>"},{"location":"experiments/encode_llm/#encode_1","title":"encode","text":"<pre><code>encode(seq: str, device: str = \"cpu\", weights_path: str = \"../../data/chkpts/squeezeprot-sp.strict/checkpoint-1383824\", tokenizer_path: str = \"../../data/tokenizer/bert-base-cased/tokenizer.t0.s8675309\")\n</code></pre> Argument Default Type Description <code>sequence</code> None String Amino-acid sequence to embed. <code>device</code> \"cpu\" String Which device to run this on. Must be a valid PyTorch device string. <code>weights_path</code> <code>\"../../data/chkpts/squeezeprot-sp.strict/checkpoint-1383824\"</code> String Path to the SqueezeProt weights to load. These are specific to the SqueezeProt variant. See the Data section for more. <code>tokenizer_path</code> <code>\"../../data/tokenizer/bert-base-cased/tokenizer.t0.s8675309\"</code> String Path to the tokenizer to load. This is the same value for all the variants. See the Data section for more."},{"location":"experiments/encode_llm/#encode_batch_1","title":"encode_batch","text":"<pre><code>encode_batch(batch: List[str], device: str = 'cpu') -&gt; List[List[float]]\n</code></pre> Argument Default Type Description <code>batch</code> None List[str] A list of amino-acid sequences to embed. <code>device</code> \"cpu\" String Which device to run this on. Must be a valid PyTorch device string. <code>weights_path</code> <code>\"../../data/chkpts/squeezeprot-sp.strict/checkpoint-1383824\"</code> String Path to the SqueezeProt weights to load. These are specific to the SqueezeProt variant. See the Data section for more. <code>tokenizer_path</code> <code>\"../../data/tokenizer/bert-base-cased/tokenizer.t0.s8675309\"</code> String Path to the tokenizer to load. This is the same value for all the variants. See the Data section for more."},{"location":"experiments/encode_llm/#requirements","title":"Requirements","text":"<p>One can install the requirements for this module using the <code>requirements.txt</code> file in the <code>experiments/encode_llm</code> folder.</p>"},{"location":"experiments/kw_bench/","title":"kw_bench","text":"<p>This folder contains the scripts required to train the SqueezeProt UniProt keyword annotation models found in the manuscript.</p>"},{"location":"experiments/kw_bench/#make_datasetpy","title":"make_dataset.py","text":"<p>A pre-computed dataset is already present in the <code>data</code> folder, you can also re-compute it if you so desire by using the <code>make_dataset.py</code> CLI tool in the <code>scripts</code> folder.</p> <p>Info</p> <p>Before re-computing the dataset for the kw_bench experiment, one must either run the embeddings_dbs experiment on all SWISS-PROT proteins or use the pre-computed vectors found in the data/embeddings folder (the default).</p> <pre><code>python make_dataset.py &lt;flags&gt;\n</code></pre>"},{"location":"experiments/kw_bench/#arguments","title":"Arguments","text":""},{"location":"experiments/kw_bench/#flags","title":"Flags","text":"Short Flag Long Flag Default Description --kw_path \"../../../data/kw/kw.json.gz\" Path to the UniProt keyword annotation data. --kw_train_path \"../../../data/kw/kw_train.csv.gz\" Where to save the generated training annotation dataset. --kw_test_path \"../../../data/kw/kw_test.csv.gz\" Where to save the generated testing annotation dataset. --kw_test_vecs_path \"../../../data/kw/kw_test_vecs.csv.gz\" Where to store the pre-computed vectors for the testing proteins. --kw_train_vecs_path \"../../../data/kw/kw_train_vecs.csv.gz\" Where to store the pre-computed vectors for the testing proteins. --kw_features_path \"../../../data/kw/subloc_kw_feature.csv.gz\" Path to a CSV file with annotation data. We have prpovided this file, which was derived from the UniProt source. -e --eval_seqs_path \"../../../data/sequences/uniprot_sprot.strict.eval.fasta.gz\" Path to protein sequences found in the evaluation set. -s --strict_database_path \"../../../data/embeddings/squeezeprot-sp.strict.lmdb\" Path to the LMDB database with SqueezeProt-SP (Strict) embeddings for all UniProt proteins. -n --nonstrict_database_path \"../../../data/embeddings/squeezeprot-sp.nonstrict.lmdb\" Path to the LMDB database with SqueezeProt-SP (Strict) embeddings for all UniProt proteins."},{"location":"experiments/kw_bench/#trainpy","title":"train.py","text":"<p>To train the UniProt keyword model from the manuscript, a CLI tool (<code>train.py</code>) was created. You can find it in the <code>scripts</code> folder. Here's how to use it:</p> <pre><code>python train.py &lt;flags&gt;\n</code></pre>"},{"location":"experiments/kw_bench/#arguments_1","title":"Arguments","text":""},{"location":"experiments/kw_bench/#flags_1","title":"Flags","text":"Short Flag Long Flag Default Description -n --nl_type \"mish\" Activation function to use. Must be one of \"mish\", \"relu6\", or \"leaky_relu\". --loss_type \"asl\" What loss function to use. Must be one of \"asl\" for an Assymetric Loss, or \"bce\" for Binary Cross-Entropy loss. --label_weighting \"log\" What label weighting function to use. Must be one of \"log\" for log-scaled weighting, \"linear\" for linearly-scaled weighting, \"root\" for weighting scaled by its square root, or \"None\" for no label weighting. --train_path \"../../../data/kw/kw_train_vecs.csv.gz\" Location of the precomputed train sequence vectors. --test_path \"../../../data/kw/kw_test_vecs.csv.gz\" Location of the precomputed test sequence vectors. -m --meta_path \"../../../data/kw/kw_meta.json\" Protein keyword dataset. -h --hparams_dir \"../../../data/kw/hparams/\" Directory where hyper-parameters are stored. -c --chkpts_dir \"../../../chkpts/kw/\" Directory where checkpoints are stored. --logs_dir \"../../../chkpts/kw/logs/\" Directory where logs are stored."},{"location":"experiments/kw_bench/#testpy","title":"test.py","text":"<p>To infer on the testing set, you can run <code>test.py</code> like so:</p> <pre><code>python test.py\n</code></pre>"},{"location":"experiments/kw_bench/#requirements","title":"Requirements","text":"<p>One can install the requirements for this experiment using the <code>requirements.txt</code> file in the <code>experiments/kw_bench</code> folder.</p>"},{"location":"experiments/mutation_bench/","title":"mutation_bench","text":"<p>This folder contains the scripts required to infer the probabilities of interaction between mutatant proteins and their wild-types proteins.</p>"},{"location":"experiments/mutation_bench/#embedding-mutated-proteins","title":"Embedding Mutated Proteins","text":"<p>Provided in the data folder are pre-computed embeddings for mutated proteins. If you wish to re-compute them, you'll need to use the <code>python bench.py embed</code> command.</p> <pre><code>python bench.py embed MODEL_NAME &lt;flags&gt;\n</code></pre>"},{"location":"experiments/mutation_bench/#arguments","title":"Arguments","text":""},{"location":"experiments/mutation_bench/#positional-arguments","title":"Positional Arguments","text":"Argument Description MODEL_NAME Name of the LLM to use to embed the proteins."},{"location":"experiments/mutation_bench/#flags","title":"Flags","text":"Long Flag Default Description --batch_size 5 The size of the batches used to embed the proteins. --muts_path \"../../data/mutation/elaspic-trainin-set-interface-ids.csv\" Data on the binding affinity of mutated and wild-type protein pairs from ELASPIC2 --output_path None Where to output the embedding database."},{"location":"experiments/mutation_bench/#requirements","title":"Requirements","text":"<p>One can install the requirements for this module using the <code>requirements.txt</code> file in the <code>experiments/mutation_bench</code> folder.</p>"},{"location":"experiments/ppi_bench/","title":"ppi_bench","text":"<p>Info</p> <p>Before running the ppi_bench experiment, one must either run the embeddings_dbs experiment on the PPI dataset you are training on or use the pre-computed vectors found in the data/embeddings folder (the default).</p> <p>This folder holds the code required to train the pLLM-based PPI inference models used in the manuscript.</p>"},{"location":"experiments/ppi_bench/#trainpy","title":"train.py","text":"<p>To train the pLLM-based PPI inference models from the manuscript, a CLI tool (<code>train.py</code>) was created. Here's how to use it:</p> <pre><code>python train.py MAX_EPOCHS NUM_LAYERS DATASET_FILE DATABASE_PATH BATCH_SIZE INPUT_DIM C_LEVEL &lt;flags&gt;\n</code></pre>"},{"location":"experiments/ppi_bench/#arguments","title":"Arguments","text":""},{"location":"experiments/ppi_bench/#positional-arguments","title":"Positional Arguments","text":"Argument Description Manuscript Value MAX_EPOCHS How many epochs to train for. 100 NUM_LAYERS How many fully-connected layers in the network. 3 DATASET_FILE Path to the PPI dataset to train/test on. Must be in the RAPPPID/INTREPPPID format. <code>../../data/ppi/rapppid_[common_string_9606.protein.links.detailed.v12.0_upkb.csv]_Mz70T9t-4Y-i6jWD9sEtcjOr0X8=.h5</code> DATABASE_PATH Path to the LMDB database with corresponding protein embeddings. Various, all from the embeddings folder. DATABASE_PATH Path to the LMDB database with corresponding protein embeddings. This solely determines which pLLM the PPI model is based on. Various, all from the embeddings folder. BATCH_SIZE The number of pairs to train on at once. Increasing this value increases RAM/VRAM use. 128 INPUT_DIM The number of elements in the embeddings inputted into the network. This is determined by the embeddings in DATABASE_PATH. Corresponds to pLLM C_LEVEL Which type of dataset (C1, C2, or C3) to use from the RAPPPID PPI dataset. See Park &amp; Marcotte for details. 3"},{"location":"experiments/ppi_bench/#flags","title":"Flags","text":"Short Flag Long Flag Default Description -w --workers 16 Number of CPU threads to use. Set this to fewer threads than your CPU affords you. -p --pooling None What pooling function (if any) to apply to the embedding before inputting to the neural network. Valid values are 'None' (no pooling), 'average' (AdaptiveAvgPool1d), or 'max' (AdaptiveMaxPool1d). -s --seed 8675309 The random seed to use."},{"location":"experiments/ppi_bench/#example","title":"Example","text":"<p>To train a PPI inference model based on 6251a85a-47d0-11ee-be56-0242ac120002:9c3954907a85287215ff77e09ebbcfc6:&lt;__None__&gt; as we did in the manuscript, you could run the following Bash script:</p> <pre><code>DATASET_FILE=\"../../data/ppi/rapppid_[common_string_9606.protein.links.detailed.v12.0_upkb.csv]_Mz70T9t-4Y-i6jWD9sEtcjOr0X8=.h5\"\nPROT_T5_DB=\"../../data/embeddings/prottrans_t5.lmdb\"\nPROT_T5_DIM=1024\n\npython train.py 100 3 $DATASET_FILE $PROT_T5_DB 128 $PROT_T5_DIM 3 -s 1\n</code></pre>"},{"location":"experiments/ppi_bench/#output","title":"Output","text":"<p>When we train a model, it is assigned a model name based on the hash of its hyperparameters. Checkpoints are saved to the folder <code>../../data/chkpts/&lt;MODEL_ID&gt;/</code> in a file that ends in <code>.ckpt</code>. Also save to that folder are the hyperparameters for the model (end in <code>.json.gz</code>) and the inferred probabilities of the testing pairs (<code>.csv</code>).</p>"},{"location":"experiments/ppi_bench/#requirements","title":"Requirements","text":"<p>One can install the requirements for this module using the <code>requirements.txt</code> file in the <code>experiments/ppi_bench</code> folder.</p>"},{"location":"experiments/sars_bench/","title":"sars_bench","text":"<p>This folder contains the scripts required to infer the probabilities of interaction between SARS-CoV-2 proteins and Homo sapiens proteins.</p>"},{"location":"experiments/sars_bench/#embedding-sars-cov-2-proteins","title":"Embedding SARS-CoV-2 Proteins","text":"<p>Provided in the data folder are pre-computed embeddings for SARS-CoV-2 proteins. If you wish to re-compute them, you'll need to use the <code>python bench.py embed</code> command.</p> <pre><code>python bench.py embed MODEL_NAME &lt;flags&gt;\n</code></pre>"},{"location":"experiments/sars_bench/#arguments","title":"Arguments","text":""},{"location":"experiments/sars_bench/#positional-arguments","title":"Positional Arguments","text":"Argument Description MODEL_NAME Name of the LLM to use to embed the proteins."},{"location":"experiments/sars_bench/#flags","title":"Flags","text":"Long Flag Default Description --batch_size 5 The size of the batches used to embed the proteins. --ppi_path \"../../data/ppi/sars-cov-2/covid_ppi.csv\" File that cotains the SARS-CoV-2 v. Human PPIs. --baits_path \"../../data/ppi/sars-cov-2/baits.fasta\" The sequences of the SARS-CoV-2 proteins. --preys_path \"\"../../data/ppi/sars-cov-2/preys.fasta\" The sequences of the Human proteins."},{"location":"experiments/sars_bench/#infer-sars-cov-2-v-human-ppis","title":"Infer SARS-CoV-2 v. Human PPIs","text":"<p>To infer SARS-CoV-2 interactions with human proteins, you'll need to use the <code>python bench.py infer</code> command.</p> <pre><code>python bench.py infer MODEL_NAME CHECKPOINT_PATH &lt;flags&gt;\n</code></pre>"},{"location":"experiments/sars_bench/#positional-arguments_1","title":"Positional Arguments","text":"Argument Description MODEL_NAME Name of the LLM to use to infer the PPIs. CHECKPOINT_PATH Path to the pLLM-based PPI inference model checkpoint. Short Flag Long Flag Default Description -o --out_dir \"../../data/ppi/sars-cov-2\" The size of the batches used to embed the proteins. -p --ppi_path \"../../data/ppi/sars-cov-2/covid_ppi.csv\" File that cotains the SARS-CoV-2 v. Human PPIs. -d --db_path <code>None</code> Path to the database that contains the protein embeddings."},{"location":"experiments/sars_bench/#requirements","title":"Requirements","text":"<p>One can install the requirements for this experiment using the <code>requirements.txt</code> file in the <code>experiments/sars_bench</code> folder.</p>"},{"location":"experiments/speed_bench/","title":"speed_bench","text":"<p>This folder contains the scripts required to measure the speed of the various pLLMs.</p>"},{"location":"experiments/speed_bench/#measuring-speed","title":"Measuring Speed","text":"<p>To run the speed benchmark you must use the <code>python bench.py run</code> command.</p>"},{"location":"experiments/speed_bench/#arguments","title":"Arguments","text":""},{"location":"experiments/speed_bench/#positional-arguments","title":"Positional Arguments","text":"Argument Description MODEL_NAME Name of the LLM to benchmark. FASTA_PATH Path to the sequences which are used to test."},{"location":"experiments/speed_bench/#flags","title":"Flags","text":"Long Flag Default Description --batch_size None The size of the batch to use in the benchmark. If left as None, the largest possible batch size that fits in memory will be used."},{"location":"experiments/speed_bench/#requirements","title":"Requirements","text":"<p>This script only requires three external libraries:</p> <ul> <li>more_itertools</li> <li>tqdm</li> <li>fire</li> </ul>"}]}